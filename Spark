// Initializing Spark in Scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)

// Sbt build file
name := "learning-spark-mini-example"
version := "0.0.1"
scalaVersion := "2.10.4"
// additional libraries
libraryDependencies ++= Seq(
"org.apache.spark" %% "spark-core" % "1.2.0" % "provided"
)

// Scala build and run
sbt clean package
$SPARK_HOME/bin/spark-submit \
—class com.oreilly.learningsparkexamples.mini.scala.WordCount \
./target/…(as above) \
./README.md ./wordcounts

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

val lines = sc.textFile("file:/home/cloudera/Desktop/dept.txt") // Create an RDD called lines
lines.count() // Count the number of items in this RDD
lines.first() // First item in this RDD, i.e. first line of README.md

RDD.persist()  // cache() is the same as calling persist() with the default storage level

//Creating RDDs
val lines = sc.parallelize(List("pandas", "i like pandas"))
val lines = sc.textFile("/path/to/README.md")

//Transformations : Transformations are operations on RDDs that return a new RDD.
// a. filter() and union()
val inputRDD = sc.textFile("log.txt")
val errorsRDD = inputRDD.filter(line => line.contains("error"))
val warningsRDD = inputRDD.filter(line => line.contains("warning"))
badLinesRDD = errorsRDD.union(warningsRDD)

// b. Map() : We can use map() to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. It

val input = sc.parallelize(List(1, 2, 3, 4))
val result = input.map(x => x * x) //Scala squaring the values in an RDD
println(result.collect().mkString(","))

val lines = sc.parallelize(List("hello world", "hi"))
val words = lines.flatMap(line => line.split(" ")) // flatMap() : splitting lines into multiple words
words.first() // returns "hello"





//Actions: These are RDD operations that return a final value to the driver program or write data to an external storage system.
// take() : used take() to retrieve a small number of elements in the RDD
badLinesRDD.take(10).foreach(println)

// collect() : RDDs function to retrieve the entire RDD
badLinesRDD.collect()

// saveAsTextFile() action, saveAsSequenceFile(), or any of a number of actions for various built-in formats




// Scala function passing
class SearchFunctions(val query: String) {
def isMatch(s: String): Boolean = {
s.contains(query)
}
def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {
// Problem: "isMatch" means "this.isMatch", so we pass all of "this"
rdd.map(isMatch)
}
def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {
// Problem: "query" means "this.query", so we pass all of "this"
rdd.map(x => x.split(query))
}
def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {
// Safe: extract just the field we need into a local variable
val query_ = this.query
rdd.map(x => x.split(query_))
}
}

